{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7960124,"sourceType":"datasetVersion","datasetId":4578667}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tqdm\n!pip install torch-summary\n!pip install -U scikit-learn\n!pip install tensorboard","metadata":{"execution":{"iopub.status.busy":"2024-04-03T20:12:30.885076Z","iopub.execute_input":"2024-04-03T20:12:30.885479Z","iopub.status.idle":"2024-04-03T20:13:27.831117Z","shell.execute_reply.started":"2024-04-03T20:12:30.885447Z","shell.execute_reply":"2024-04-03T20:13:27.829589Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.1)\nRequirement already satisfied: torch-summary in /opt/conda/lib/python3.10/site-packages (1.4.5)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.4.1.post1)\nRequirement already satisfied: numpy<2.0,>=1.19.5 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\nRequirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.4)\nRequirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\nRequirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.15.1)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.51.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.5.2)\nRequirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.26.4)\nRequirement already satisfied: protobuf<4.24,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.31.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (69.0.3)\nRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.16.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.0.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"SHOULD_PRINT = True\nSEED = 32\nCONTINUE_MODEL = None","metadata":{"execution":{"iopub.status.busy":"2024-04-03T20:13:27.834599Z","iopub.execute_input":"2024-04-03T20:13:27.835112Z","iopub.status.idle":"2024-04-03T20:13:27.841744Z","shell.execute_reply.started":"2024-04-03T20:13:27.835069Z","shell.execute_reply":"2024-04-03T20:13:27.840524Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\n\ntorch.manual_seed(SEED)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T20:13:27.843745Z","iopub.execute_input":"2024-04-03T20:13:27.844145Z","iopub.status.idle":"2024-04-03T20:13:27.855940Z","shell.execute_reply.started":"2024-04-03T20:13:27.844110Z","shell.execute_reply":"2024-04-03T20:13:27.854848Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7cde46162ab0>"},"metadata":{}}]},{"cell_type":"code","source":"DEVICE = \"cpu\"\nif torch.cuda.is_available():\n    DEVICE = \"cuda\"","metadata":{"execution":{"iopub.status.busy":"2024-04-03T20:13:27.857150Z","iopub.execute_input":"2024-04-03T20:13:27.857590Z","iopub.status.idle":"2024-04-03T20:13:27.868202Z","shell.execute_reply.started":"2024-04-03T20:13:27.857553Z","shell.execute_reply":"2024-04-03T20:13:27.867081Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom typing import List\nfrom torch.utils.data import Dataset\nfrom sklearn.preprocessing import StandardScaler\nimport random\n\nclass CustomDataset(Dataset):\n    def __init__(self, large_file_path, chunk_size, subset_size=50000):\n        self.large_file_path = large_file_path\n        self.chunk_size = chunk_size\n        self.subset_size = subset_size\n        self.line_offsets = self.get_line_offsets(large_file_path, chunk_size)\n        self.scaler = StandardScaler()\n        print(\"Calculating mean and std...\")\n        self.mean, self.std = self.calculate_mean_std()\n        print(f\"Mean: {self.mean}, Std: {self.std}\")\n\n    def get_line_offsets(self, path: str, chunk_size: int) -> List[int]:\n        offsets = [0]\n        with open(path, \"rb\") as file:\n            chunk = file.readlines(chunk_size)\n            while chunk:\n                for line in chunk:\n                    offsets.append(offsets[-1] + len(line))\n                chunk = file.readlines(chunk_size)\n                print(f\"Lines found: {len(offsets)}\", end='\\r')\n        offsets = offsets[:-1]\n        print(f\"Lines found: {len(offsets)}\", end='\\n')\n        return offsets\n\n    def calculate_mean_std(self):\n        selected_offsets = random.sample(self.line_offsets, min(self.subset_size, len(self.line_offsets)))\n        features = []\n        for offset in selected_offsets:\n            with open(self.large_file_path, 'r', encoding='utf-8') as f:\n                f.seek(offset)\n                line = f.readline()\n                numbers = [float(num) for num in line.strip().split()]\n                features.append(numbers[:4])\n        features = np.array(features)\n        mean = np.mean(features, axis=0, dtype=np.float32)\n        std = np.std(features, axis=0, dtype=np.float32)\n        return mean, std\n\n    def standardize_features(self, features):\n        standardized_features = (features - self.mean) / self.std\n        return standardized_features\n\n    def __len__(self):\n        return len(self.line_offsets)\n\n    def __getitem__(self, line):\n        offset = self.line_offsets[line]\n        with open(self.large_file_path, 'r', encoding='utf-8') as f:\n            f.seek(offset)\n            line = f.readline()\n            numbers = [float(num) for num in line.strip().split()]\n            features, targets = numbers[:4], numbers[4:]\n            standardized_features = self.standardize_features(np.array(features))\n            return torch.tensor(standardized_features, dtype=torch.float32), torch.tensor(targets, dtype=torch.float32)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-03T20:13:27.872008Z","iopub.execute_input":"2024-04-03T20:13:27.872398Z","iopub.status.idle":"2024-04-03T20:13:27.895423Z","shell.execute_reply.started":"2024-04-03T20:13:27.872364Z","shell.execute_reply":"2024-04-03T20:13:27.894347Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"filename = \"/kaggle/input/harmonic/full_dataset.txt\"\nfull_dataset = CustomDataset(filename, 2**20)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T20:13:27.897278Z","iopub.execute_input":"2024-04-03T20:13:27.897813Z","iopub.status.idle":"2024-04-03T20:14:03.527025Z","shell.execute_reply.started":"2024-04-03T20:13:27.897775Z","shell.execute_reply":"2024-04-03T20:14:03.525878Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Lines found: 52051720\nCalculating mean and std...\nMean: [ 6.6733383e+01 -5.4159999e-02  1.5670270e+00  3.1063519e+00], Std: [23.899603   41.23458     0.92115563  1.8126098 ]\n","output_type":"stream"}]},{"cell_type":"code","source":"train_size = int(0.0001 * len(full_dataset))\nrest_size = len(full_dataset) - train_size\nval_size = rest_size // 10000\ntest_size = rest_size - val_size\ntrain_dataset, val_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(SEED))","metadata":{"execution":{"iopub.status.busy":"2024-04-03T20:14:03.528498Z","iopub.execute_input":"2024-04-03T20:14:03.528839Z","iopub.status.idle":"2024-04-03T20:14:10.923807Z","shell.execute_reply.started":"2024-04-03T20:14:03.528810Z","shell.execute_reply":"2024-04-03T20:14:10.922860Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"train_size","metadata":{"execution":{"iopub.status.busy":"2024-04-03T20:14:10.925073Z","iopub.execute_input":"2024-04-03T20:14:10.925410Z","iopub.status.idle":"2024-04-03T20:14:10.932290Z","shell.execute_reply.started":"2024-04-03T20:14:10.925383Z","shell.execute_reply":"2024-04-03T20:14:10.931110Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"5205"},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nBATCH_SIZE = 2 ** 22\ntrain_shuffle = True\nval_shuffle = False\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=train_shuffle)\nval_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=val_shuffle)\ntest_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T20:14:10.933713Z","iopub.execute_input":"2024-04-03T20:14:10.934036Z","iopub.status.idle":"2024-04-03T20:14:12.069721Z","shell.execute_reply.started":"2024-04-03T20:14:10.934010Z","shell.execute_reply":"2024-04-03T20:14:12.068699Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.init as init","metadata":{"execution":{"iopub.status.busy":"2024-04-03T20:14:12.071067Z","iopub.execute_input":"2024-04-03T20:14:12.071449Z","iopub.status.idle":"2024-04-03T20:14:12.080240Z","shell.execute_reply.started":"2024-04-03T20:14:12.071420Z","shell.execute_reply":"2024-04-03T20:14:12.079416Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"class Block(nn.Module):\n    def __init__(self, in_layers, out_layers):\n        super(Block, self).__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(in_layers, out_layers),\n            nn.ReLU(),\n            nn.Dropout1d(p=0.1),\n        )\n        \n    def forward(self, x):\n        x = self.layers(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-04-03T20:14:12.081556Z","iopub.execute_input":"2024-04-03T20:14:12.081869Z","iopub.status.idle":"2024-04-03T20:14:12.090413Z","shell.execute_reply.started":"2024-04-03T20:14:12.081844Z","shell.execute_reply":"2024-04-03T20:14:12.089457Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self):\n        super(Encoder, self).__init__()\n        self.blocks = nn.ModuleList([\n            Block(4, 256),\n            Block(256, 256),\n            Block(256, 256),\n        ])\n        self.bottleneck = nn.ModuleList([\n            Block(256, 256),\n            nn.Linear(256, 4),\n        ])\n        \n        self.__init_weights()\n\n    def __init_weights(self):\n        layers = [self.blocks, self.bottleneck]\n        # Initialize linear layers using Kaiming (He) uniform initialization\n        for m in layers:\n            for layer in m:\n                self.__init_layer(layer)\n                        \n    def __init_layer(self, layer):\n        if isinstance(layer, nn.Linear):\n            init.kaiming_uniform_(layer.weight, mode='fan_in', nonlinearity='tanh')\n            if layer.bias is not None:\n                init.zeros_(layer.bias)\n\n    def forward(self, x):\n        x = self.blocks[0](x)\n        for block in self.blocks[1:]:\n            y = block(x)\n            x = x + y\n        for btl in self.bottleneck:\n            x = btl(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-04-03T20:14:12.091770Z","iopub.execute_input":"2024-04-03T20:14:12.092521Z","iopub.status.idle":"2024-04-03T20:14:12.102514Z","shell.execute_reply.started":"2024-04-03T20:14:12.092486Z","shell.execute_reply":"2024-04-03T20:14:12.101566Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self):\n        super(Decoder, self).__init__()\n        self.blocks = nn.ModuleList([\n            Block(4, 512),\n            Block(512, 512),\n        ])\n        self.out = nn.Linear(512, 2)\n        self.__init_weights()\n\n    def __init_weights(self):\n        layers = [self.blocks]\n        # Initialize linear layers using Kaiming (He) uniform initialization\n        for m in layers:\n            for layer in m:\n                self.__init_layer(layer)\n        self.__init_layer(self.out)\n                        \n    def __init_layer(self, layer):\n        if isinstance(layer, nn.Linear):\n            init.kaiming_uniform_(layer.weight, mode='fan_in', nonlinearity='tanh')\n            if layer.bias is not None:\n                init.zeros_(layer.bias)\n\n    def forward(self, x):\n        x = self.blocks[0](x)\n        for block in self.blocks[1:]:\n            y = block(x)\n            x = x + y\n        x = self.out(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-04-03T20:35:44.854395Z","iopub.execute_input":"2024-04-03T20:35:44.855150Z","iopub.status.idle":"2024-04-03T20:35:44.865373Z","shell.execute_reply.started":"2024-04-03T20:35:44.855119Z","shell.execute_reply":"2024-04-03T20:35:44.864279Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"class RegressionAutoencoder(nn.Module):\n    def __init__(self, encoder, decoder, freeze):\n        super(RegressionAutoencoder, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.gradient(self.encoder, freeze)\n        \n    def gradient(self, model, freeze: bool):\n        for parameter in model.parameters():\n            parameter.requires_grad_(not freeze)\n        \n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-04-03T20:36:46.949732Z","iopub.execute_input":"2024-04-03T20:36:46.950113Z","iopub.status.idle":"2024-04-03T20:36:46.958296Z","shell.execute_reply.started":"2024-04-03T20:36:46.950086Z","shell.execute_reply":"2024-04-03T20:36:46.957014Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"# Methods","metadata":{}},{"cell_type":"code","source":"from torch.utils.tensorboard import SummaryWriter\n\ndef train(is_encoder, model, dataloader, optimizer, scheduler, loss_fn, epoch, writer, log_perc = 0.1):\n    model.train()\n    total_loss = 0\n    total_diff = 0\n    best_diff = 100\n\n    logs_steps = max(int(log_perc * len(dataloader)), 1)\n    start_step = epoch * len(dataloader)\n\n    before_lr = optimizer.param_groups[0][\"lr\"]\n    writer.add_scalar('Lr/Train', before_lr, epoch)\n    for idx, (inputs, targets) in enumerate(tqdm(dataloader)):\n        inputs, targets = inputs.to(DEVICE).to(torch.float32), targets.to(DEVICE).to(torch.float32)\n        \n        if is_encoder:\n            targets = inputs\n        \n        optimizer.zero_grad()\n\n        outputs = model(inputs)\n\n        loss = loss_fn(outputs, targets)\n        diff = torch.abs(outputs - targets).mean()\n\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        total_loss += loss.item()\n        total_diff += diff.item()\n        \n        if idx % logs_steps == 0:\n            writer.add_scalar('Loss/Train', loss.item(), start_step + idx)\n            writer.add_scalar('Absolute Difference/Train', diff.item(), start_step + idx)\n            \n            if SHOULD_PRINT:\n                print(f\"Loss/Train: {loss.item()}\")\n                print(f\"Absolute Difference/Train: {diff.item()}\")\n        \n    after_lr = optimizer.param_groups[0][\"lr\"]\n    average_loss = total_loss / len(dataloader)\n    average_diff = total_diff / len(dataloader)\n    \n    writer.add_scalar('Avg Loss/Train', average_loss, epoch)\n    writer.add_scalar('Avg Absolute Difference/Train', average_diff, epoch)\n    writer.add_scalar('Lr/Train', after_lr, epoch)\n    \n    if SHOULD_PRINT:\n        print(f\"Avg Loss/Train: {average_loss}\")\n        print(f\"Avg Absolute Difference/Train: {average_diff}\")\n        print(f\"Lr/Train: {after_lr}\")\n\n    # print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Train Loss: {average_loss:.4f}, Train Diff: {average_diff:.15f}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-03T20:14:12.132455Z","iopub.execute_input":"2024-04-03T20:14:12.132821Z","iopub.status.idle":"2024-04-03T20:14:12.563393Z","shell.execute_reply.started":"2024-04-03T20:14:12.132796Z","shell.execute_reply":"2024-04-03T20:14:12.562161Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"def validate(is_encoder, model, dataloader, loss_fn, epoch, writer):\n    model.eval()\n    total_loss = 0\n    total_diff = 0\n\n    with torch.no_grad():\n        for inputs, targets in dataloader:\n            inputs, targets = inputs.to(DEVICE).to(torch.float32), targets.to(DEVICE).to(torch.float32)\n            \n            if is_encoder:\n                targets = inputs\n            \n            outputs = model(inputs)\n\n            loss = loss_fn(outputs, targets)\n            diff = torch.abs(outputs - targets).mean()\n            \n            total_loss += loss.item()\n            total_diff += diff.item()\n\n    average_loss = total_loss / len(dataloader)\n    average_diff = total_diff / len(dataloader)\n\n    if writer is not None:\n        writer.add_scalar('Avg Loss/Val', average_loss, epoch)\n        writer.add_scalar('Avg Absolute Difference/Val', average_diff, epoch)\n    \n    if SHOULD_PRINT:\n        print(f\"Avg Loss/Val: {average_loss}\")\n        print(f\"Avg Absolute Difference/Val: {average_diff}\")\n\n    if epoch is not None:\n        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Val Loss: {average_loss:.4f}, Val Diff: {average_diff:.15f}\")\n    else:\n        print(f\"Test Loss: {average_loss:.4f}, Test Diff: {average_diff:.15f}\")\n        \n    return average_diff","metadata":{"execution":{"iopub.status.busy":"2024-04-03T20:18:04.831236Z","iopub.execute_input":"2024-04-03T20:18:04.831678Z","iopub.status.idle":"2024-04-03T20:18:04.843233Z","shell.execute_reply.started":"2024-04-03T20:18:04.831632Z","shell.execute_reply":"2024-04-03T20:18:04.841719Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"# Encoder Training","metadata":{}},{"cell_type":"code","source":"LR = 1e-3\nNUM_EPOCHS = 1\nWEIGHT_DECAY = 0.99\nWEIGHT_DECAY_L1 = 1e-4\nMOMENTUM = 0.9","metadata":{"execution":{"iopub.status.busy":"2024-04-03T20:18:06.123815Z","iopub.execute_input":"2024-04-03T20:18:06.124476Z","iopub.status.idle":"2024-04-03T20:18:06.129272Z","shell.execute_reply.started":"2024-04-03T20:18:06.124444Z","shell.execute_reply":"2024-04-03T20:18:06.128231Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\nimport torch.optim.lr_scheduler as lr_scheduler\n\nlast_epoch = 0\nencoder_model = Encoder()\nloss_mse = nn.MSELoss()\noptimizer = optim.SGD(encoder_model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY, nesterov=True)\nscheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, verbose=True)\nif CONTINUE_MODEL:\n    checkpoint = torch.load(\"encoder_checkpoint.pth\")\n    encoder_model.load_state_dict(checkpoint['encoder_model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n    last_epoch = checkpoint['epoch']\n    loss_mse = checkpoint['loss_mse']","metadata":{"execution":{"iopub.status.busy":"2024-04-03T20:18:06.419866Z","iopub.execute_input":"2024-04-03T20:18:06.420933Z","iopub.status.idle":"2024-04-03T20:18:06.434006Z","shell.execute_reply.started":"2024-04-03T20:18:06.420896Z","shell.execute_reply":"2024-04-03T20:18:06.432941Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"Epoch 00000: adjusting learning rate of group 0 to 1.0000e-03.\n","output_type":"stream"}]},{"cell_type":"code","source":"def total_loss(outputs, targets):\n    loss = loss_mse(outputs, targets)\n#     loss_rmse = torch.sqrt(loss)\n#     l1_reg = torch.tensor(0., requires_grad=True)\n\n#     for name, param in encoder_model.named_parameters():\n#         if 'weight' in name:\n#             l1_reg = l1_reg + torch.linalg.norm(param, 1)\n\n#     total_loss = (loss * 0.8 + loss_rmse * 0.2) + WEIGHT_DECAY_L1 * l1_reg\n#     total_loss = (loss * 0.8 + loss_rmse * 0.2)\n    return loss","metadata":{"execution":{"iopub.status.busy":"2024-04-03T20:18:06.820112Z","iopub.execute_input":"2024-04-03T20:18:06.820826Z","iopub.status.idle":"2024-04-03T20:18:06.826272Z","shell.execute_reply.started":"2024-04-03T20:18:06.820791Z","shell.execute_reply":"2024-04-03T20:18:06.825056Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"import datetime\n\nnow = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\nwriter = SummaryWriter(f\"tb_logs/encoder/{now}\")\n\nVALIDATION_STEPS = 6\n\nencoder_model = encoder_model.to(DEVICE)\nbest_avg_diff = 1000\n\nfor idx, epoch in enumerate(range(last_epoch, NUM_EPOCHS)):\n    train(True, encoder_model, train_dataloader, optimizer, scheduler, total_loss, epoch, writer, log_perc=0.2)\n\n    if idx % VALIDATION_STEPS == 0:\n        average_diff = validate(True, encoder_model, val_dataloader, total_loss, epoch, writer)\n        if average_diff < best_avg_diff:\n            torch.save({\n                'epoch': epoch,\n                'encoder_model_state_dict': encoder_model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict(),\n                'loss_mse': loss_mse,\n                }, \"encoder_checkpoint.pth\")\n\n# Launch TensorBoard: `tensorboard --logdir=tb_logs`","metadata":{"execution":{"iopub.status.busy":"2024-04-03T20:18:07.235748Z","iopub.execute_input":"2024-04-03T20:18:07.236387Z","iopub.status.idle":"2024-04-03T20:18:11.568801Z","shell.execute_reply.started":"2024-04-03T20:18:07.236357Z","shell.execute_reply":"2024-04-03T20:18:11.567551Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stderr","text":"100%|██████████| 1/1 [00:02<00:00,  2.19s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 00001: adjusting learning rate of group 0 to 9.7553e-04.\nLoss/Train: 1.6271324157714844\nAbsolute Difference/Train: 1.0536119937896729\nAvg Loss/Train: 1.6271324157714844\nAvg Absolute Difference/Train: 1.0536119937896729\nLr/Train: 0.0009755282581475768\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Avg Loss/Val: 1.3871269226074219\nAvg Absolute Difference/Val: 0.9900140762329102\nEpoch [1/1] Val Loss: 1.3871, Val Diff: 0.990014076232910\n","output_type":"stream"}]},{"cell_type":"code","source":"#validate(True, encoder_model, test_dataloader, total_loss, epoch, None)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T20:18:11.570798Z","iopub.execute_input":"2024-04-03T20:18:11.571150Z","iopub.status.idle":"2024-04-03T20:34:44.939124Z","shell.execute_reply.started":"2024-04-03T20:18:11.571120Z","shell.execute_reply":"2024-04-03T20:34:44.936773Z"},"trusted":true},"execution_count":56,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[51], line 7\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(is_encoder, model, dataloader, loss_fn, epoch, writer)\u001b[0m\n\u001b[1;32m      4\u001b[0m total_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m      8\u001b[0m         inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(DEVICE)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32), targets\u001b[38;5;241m.\u001b[39mto(DEVICE)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m is_encoder:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n","Cell \u001b[0;32mIn[35], line 55\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, line):\n\u001b[1;32m     54\u001b[0m     offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline_offsets[line]\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlarge_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     56\u001b[0m         f\u001b[38;5;241m.\u001b[39mseek(offset)\n\u001b[1;32m     57\u001b[0m         line \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadline()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/codecs.py:309\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.__init__\u001b[0;34m(self, errors)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBufferedIncrementalDecoder\u001b[39;00m(IncrementalDecoder):\n\u001b[1;32m    304\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;124;03m    This subclass of IncrementalDecoder can be used as the baseclass for an\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;124;03m    incremental decoder if the decoder must be able to handle incomplete\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;124;03m    byte sequences.\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    310\u001b[0m         IncrementalDecoder\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, errors)\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;66;03m# undecoded input that is kept between calls to decode()\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"# Regression Autoencoder Model","metadata":{}},{"cell_type":"code","source":"LR = 5e-4\nNUM_EPOCHS = 1\nWEIGHT_DECAY = 0.99\nWEIGHT_DECAY_L1 = 1e-4\nMOMENTUM = 0.9","metadata":{"execution":{"iopub.status.busy":"2024-04-03T20:36:54.916928Z","iopub.execute_input":"2024-04-03T20:36:54.917315Z","iopub.status.idle":"2024-04-03T20:36:54.925912Z","shell.execute_reply.started":"2024-04-03T20:36:54.917287Z","shell.execute_reply":"2024-04-03T20:36:54.924714Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\nimport torch.optim.lr_scheduler as lr_scheduler\n\nlast_epoch = 0\ndecoder = Decoder()\nautoencoder_model = RegressionAutoencoder(encoder=encoder_model, decoder=decoder, freeze=True)\nloss_mse = nn.MSELoss()\noptimizer = optim.SGD(autoencoder_model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY, nesterov=True)\nscheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, verbose=True)\nif CONTINUE_MODEL:\n    checkpoint = torch.load(\"regressor_freezed_checkpoint.pth\")\n    autoencoder_model.load_state_dict(checkpoint['autoencoder_model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n    last_epoch = checkpoint['epoch']\n    loss_mse = checkpoint['loss_mse']","metadata":{"execution":{"iopub.status.busy":"2024-04-03T20:36:56.174026Z","iopub.execute_input":"2024-04-03T20:36:56.174428Z","iopub.status.idle":"2024-04-03T20:36:56.192350Z","shell.execute_reply.started":"2024-04-03T20:36:56.174398Z","shell.execute_reply":"2024-04-03T20:36:56.190918Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stdout","text":"Epoch 00000: adjusting learning rate of group 0 to 5.0000e-04.\n","output_type":"stream"}]},{"cell_type":"code","source":"def total_loss(outputs, targets):\n    loss = loss_mse(outputs, targets)\n#     loss_rmse = torch.sqrt(loss)\n#     l1_reg = torch.tensor(0., requires_grad=True)\n\n#     for name, param in autoencoder_model.named_parameters():\n#         if 'weight' in name:\n#             l1_reg = l1_reg + torch.linalg.norm(param, 1)\n\n#     total_loss = (loss * 0.8 + loss_rmse * 0.2) + WEIGHT_DECAY_L1 * l1_reg\n#     total_loss = (loss * 0.8 + loss_rmse * 0.2)\n    return loss","metadata":{"execution":{"iopub.status.busy":"2024-04-03T20:37:02.289445Z","iopub.execute_input":"2024-04-03T20:37:02.290165Z","iopub.status.idle":"2024-04-03T20:37:02.294929Z","shell.execute_reply.started":"2024-04-03T20:37:02.290130Z","shell.execute_reply":"2024-04-03T20:37:02.293959Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"import datetime\n\nnow = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\nwriter = SummaryWriter(f\"tb_logs/regressor/{now}\")\n\nVALIDATION_STEPS = 6\n\nautoencoder_model = autoencoder_model.to(DEVICE)\nbest_avg_diff = 1000\n\nfor idx, epoch in enumerate(range(last_epoch, NUM_EPOCHS)):\n    train(False, autoencoder_model, train_dataloader, optimizer, scheduler, total_loss, epoch, writer, log_perc=0.2)\n\n    if idx % VALIDATION_STEPS == 0:\n        average_diff = validate(False, autoencoder_model, val_dataloader, total_loss, epoch, writer)\n        if average_diff < best_avg_diff:\n            torch.save({\n                'epoch': epoch,\n                'autoencoder_model_state_dict': autoencoder_model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict(),\n                'loss_mse': loss_mse,\n                }, \"regressor_freezed_checkpoint.pth\")\n\n# Launch TensorBoard: `tensorboard --logdir=tb_logs`","metadata":{"execution":{"iopub.status.busy":"2024-04-03T20:37:03.621757Z","iopub.execute_input":"2024-04-03T20:37:03.622155Z","iopub.status.idle":"2024-04-03T20:37:07.506692Z","shell.execute_reply.started":"2024-04-03T20:37:03.622125Z","shell.execute_reply":"2024-04-03T20:37:07.505677Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stderr","text":"100%|██████████| 1/1 [00:01<00:00,  1.92s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 00001: adjusting learning rate of group 0 to 4.8776e-04.\nLoss/Train: 0.5163660645484924\nAbsolute Difference/Train: 0.5990405082702637\nAvg Loss/Train: 0.5163660645484924\nAvg Absolute Difference/Train: 0.5990405082702637\nLr/Train: 0.0004877641290737884\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Avg Loss/Val: 0.3422410190105438\nAvg Absolute Difference/Val: 0.49869897961616516\nEpoch [1/1] Val Loss: 0.3422, Val Diff: 0.498698979616165\n","output_type":"stream"}]},{"cell_type":"code","source":"#validate(False, autoencoder_model, test_dataloader, total_loss, epoch, None)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T20:37:07.508286Z","iopub.execute_input":"2024-04-03T20:37:07.508609Z","iopub.status.idle":"2024-04-03T20:37:07.512673Z","shell.execute_reply.started":"2024-04-03T20:37:07.508584Z","shell.execute_reply":"2024-04-03T20:37:07.511679Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":"# Unfreezed Regression Autoencoder Model","metadata":{}},{"cell_type":"code","source":"LR = 5e-4\nNUM_EPOCHS = 1\nWEIGHT_DECAY = 0.99\nWEIGHT_DECAY_L1 = 1e-4\nMOMENTUM = 0.9","metadata":{"execution":{"iopub.status.busy":"2024-04-03T20:37:08.990633Z","iopub.execute_input":"2024-04-03T20:37:08.991559Z","iopub.status.idle":"2024-04-03T20:37:08.996192Z","shell.execute_reply.started":"2024-04-03T20:37:08.991526Z","shell.execute_reply":"2024-04-03T20:37:08.995177Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\nimport torch.optim.lr_scheduler as lr_scheduler\n\nlast_epoch = 0\ndecoder = Decoder()\nautoencoder_model = RegressionAutoencoder(encoder=encoder_model, decoder=decoder, freeze=False)\nloss_mse = nn.MSELoss()\noptimizer = optim.SGD(autoencoder_model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY, nesterov=True)\nscheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, verbose=True)\nif CONTINUE_MODEL:\n    checkpoint = torch.load(\"regressor_unfreezed_checkpoint.pth\")\n    autoencoder_model.load_state_dict(checkpoint['autoencoder_model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n    last_epoch = checkpoint['epoch']\n    loss_mse = checkpoint['loss_mse']","metadata":{"execution":{"iopub.status.busy":"2024-04-03T20:37:21.160734Z","iopub.execute_input":"2024-04-03T20:37:21.161166Z","iopub.status.idle":"2024-04-03T20:37:21.176377Z","shell.execute_reply.started":"2024-04-03T20:37:21.161132Z","shell.execute_reply":"2024-04-03T20:37:21.175158Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stdout","text":"Epoch 00000: adjusting learning rate of group 0 to 5.0000e-04.\n","output_type":"stream"}]},{"cell_type":"code","source":"def total_loss(outputs, targets):\n    loss = loss_mse(outputs, targets)\n#     loss_rmse = torch.sqrt(loss)\n#     l1_reg = torch.tensor(0., requires_grad=True)\n\n#     for name, param in autoencoder_model.named_parameters():\n#         if 'weight' in name:\n#             l1_reg = l1_reg + torch.linalg.norm(param, 1)\n\n#     total_loss = (loss * 0.8 + loss_rmse * 0.2) + WEIGHT_DECAY_L1 * l1_reg\n#     total_loss = (loss * 0.8 + loss_rmse * 0.2)\n    return loss","metadata":{"execution":{"iopub.status.busy":"2024-04-03T20:37:22.039754Z","iopub.execute_input":"2024-04-03T20:37:22.040471Z","iopub.status.idle":"2024-04-03T20:37:22.045800Z","shell.execute_reply.started":"2024-04-03T20:37:22.040433Z","shell.execute_reply":"2024-04-03T20:37:22.044641Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"import datetime\n\nnow = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\nwriter = SummaryWriter(f\"tb_logs/regressor/{now}\")\n\nVALIDATION_STEPS = 6\n\nautoencoder_model = autoencoder_model.to(DEVICE)\nbest_avg_diff = 1000\n\nfor idx, epoch in enumerate(range(last_epoch, NUM_EPOCHS)):\n    train(False, autoencoder_model, train_dataloader, optimizer, scheduler, total_loss, epoch, writer, log_perc=0.2)\n\n    if idx % VALIDATION_STEPS == 0:\n        average_diff = validate(False, autoencoder_model, val_dataloader, total_loss, epoch, writer)\n        if average_diff < best_avg_diff:\n            torch.save({\n                'epoch': epoch,\n                'autoencoder_model_state_dict': autoencoder_model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict(),\n                'loss_mse': loss_mse,\n                }, \"regressor_unfreezed_checkpoint.pth\")\n\n# Launch TensorBoard: `tensorboard --logdir=tb_logs`","metadata":{"execution":{"iopub.status.busy":"2024-04-03T20:37:26.156884Z","iopub.execute_input":"2024-04-03T20:37:26.157701Z","iopub.status.idle":"2024-04-03T20:37:30.369239Z","shell.execute_reply.started":"2024-04-03T20:37:26.157663Z","shell.execute_reply":"2024-04-03T20:37:30.367954Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stderr","text":"100%|██████████| 1/1 [00:02<00:00,  2.06s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 00001: adjusting learning rate of group 0 to 4.8776e-04.\nLoss/Train: 0.21327556669712067\nAbsolute Difference/Train: 0.33251211047172546\nAvg Loss/Train: 0.21327556669712067\nAvg Absolute Difference/Train: 0.33251211047172546\nLr/Train: 0.0004877641290737884\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Avg Loss/Val: 0.12447430193424225\nAvg Absolute Difference/Val: 0.2655174136161804\nEpoch [1/1] Val Loss: 0.1245, Val Diff: 0.265517413616180\n","output_type":"stream"}]},{"cell_type":"code","source":"#validate(False, autoencoder_model, test_dataloader, total_loss, epoch, None)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T20:34:44.958580Z","iopub.status.idle":"2024-04-03T20:34:44.959009Z","shell.execute_reply.started":"2024-04-03T20:34:44.958802Z","shell.execute_reply":"2024-04-03T20:34:44.958819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}