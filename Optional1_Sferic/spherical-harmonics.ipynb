{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-20T19:40:49.953568Z","iopub.status.busy":"2024-03-20T19:40:49.953109Z","iopub.status.idle":"2024-03-20T19:40:49.958969Z","shell.execute_reply":"2024-03-20T19:40:49.957744Z","shell.execute_reply.started":"2024-03-20T19:40:49.953531Z"},"trusted":true},"outputs":[],"source":["SHOULD_PRINT = False"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-20T19:40:49.961234Z","iopub.status.busy":"2024-03-20T19:40:49.960797Z","iopub.status.idle":"2024-03-20T19:40:49.971713Z","shell.execute_reply":"2024-03-20T19:40:49.970696Z","shell.execute_reply.started":"2024-03-20T19:40:49.961177Z"},"trusted":true},"outputs":[],"source":["import torch\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-20T19:40:49.974652Z","iopub.status.busy":"2024-03-20T19:40:49.974271Z","iopub.status.idle":"2024-03-20T19:40:49.985176Z","shell.execute_reply":"2024-03-20T19:40:49.984414Z","shell.execute_reply.started":"2024-03-20T19:40:49.974620Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset\n","from typing import List\n","import random\n","\n","random.seed(42)\n","\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, large_file_path, chunk_size, dataset_size=None):\n","        self.large_file_path = large_file_path\n","        self.line_offsets = self.get_line_offsets(\n","            large_file_path, chunk_size, dataset_size\n","        )\n","\n","    def get_line_offsets(\n","        self, path: str, chunk_size: int, dataset_size: int\n","    ) -> List[int]:\n","        offsets = [0]\n","        with open(path, \"rb\") as file:\n","            chunk = file.readlines(chunk_size)\n","            while chunk:\n","                for line in chunk:\n","                    offsets.append(offsets[-1] + len(line))\n","                chunk = file.readlines(chunk_size)\n","        offsets = offsets[:-1]\n","\n","        if dataset_size is not None:\n","            offsets = random.sample(offsets, dataset_size)\n","\n","        return offsets\n","\n","    def __len__(self):\n","        return len(self.line_offsets)\n","\n","    def __getitem__(self, line):\n","        offset = self.line_offsets[line]\n","        with open(self.large_file_path, \"r\", encoding=\"utf-8\") as f:\n","            f.seek(offset)\n","            line = f.readline()\n","            numbers = [float(num) for num in line.strip().split()]\n","            inputs, targets = numbers[:4], numbers[4:]\n","            return torch.tensor(inputs), torch.tensor(targets)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-20T19:40:49.986967Z","iopub.status.busy":"2024-03-20T19:40:49.986671Z","iopub.status.idle":"2024-03-20T19:40:50.395712Z","shell.execute_reply":"2024-03-20T19:40:50.394733Z","shell.execute_reply.started":"2024-03-20T19:40:49.986943Z"},"trusted":true},"outputs":[],"source":["filename = \"./data/full_dataset.txt\"\n","full_dataset = CustomDataset(filename, chunk_size=2**20, dataset_size=50000)\n","# filename = \"./data/sph_100_10_20.txt\"\n","# full_dataset = CustomDataset(filename, chunk_size=2**20)\n","\n","train_size = int(0.8 * len(full_dataset))\n","rest_size = len(full_dataset) - train_size\n","val_size = rest_size // 2\n","test_size = rest_size // 2\n","train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n","    full_dataset,\n","    [train_size, val_size, test_size],\n","    generator=torch.Generator().manual_seed(32),\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-20T19:40:50.397833Z","iopub.status.busy":"2024-03-20T19:40:50.397454Z","iopub.status.idle":"2024-03-20T19:40:50.410448Z","shell.execute_reply":"2024-03-20T19:40:50.409141Z","shell.execute_reply.started":"2024-03-20T19:40:50.397804Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","BATCH_SIZE = 1024\n","train_shuffle = True\n","val_shuffle = False\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=train_shuffle)\n","val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=val_shuffle)\n","test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-20T19:40:50.653427Z","iopub.status.busy":"2024-03-20T19:40:50.652941Z","iopub.status.idle":"2024-03-20T19:40:50.663146Z","shell.execute_reply":"2024-03-20T19:40:50.662188Z","shell.execute_reply.started":"2024-03-20T19:40:50.653393Z"},"trusted":true},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.init as init\n","\n","class MLP(nn.Module):\n","    def __init__(self):\n","        super(MLP, self).__init__()\n","        # self.layers = nn.Sequential(\n","        #     nn.Linear(4, 128),\n","        #     nn.Tanh(),\n","        #     nn.Linear(128, 256),\n","        #     nn.ReLU(),\n","        #     nn.Linear(256, 512),\n","        #     nn.ReLU(),\n","        #     nn.Linear(512, 512),\n","        #     nn.ReLU(),\n","        #     nn.Linear(512, 256),\n","        #     nn.ReLU(),\n","        #     nn.Linear(256, 128),\n","        #     nn.Tanh(),\n","        #     nn.Linear(128, 2)\n","        # )\n","        self.layers = nn.Sequential(\n","            nn.Linear(4, 128),\n","            nn.Tanh(),\n","            nn.Linear(128, 128),\n","            nn.Tanh(),\n","            nn.Linear(128, 2)\n","        )\n","        self.__init_weights()\n","\n","    def __init_weights(self):\n","        # Initialize linear layers using Kaiming (He) uniform initialization\n","        for m in self.layers.children():\n","            if isinstance(m, nn.Linear):\n","                init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='tanh')\n","                if m.bias is not None:\n","                    init.zeros_(m.bias)\n","\n","    def forward(self, x):\n","        return self.layers(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-20T19:40:50.664838Z","iopub.status.busy":"2024-03-20T19:40:50.664219Z","iopub.status.idle":"2024-03-20T19:40:50.679723Z","shell.execute_reply":"2024-03-20T19:40:50.678290Z","shell.execute_reply.started":"2024-03-20T19:40:50.664797Z"},"trusted":true},"outputs":[],"source":["LR = 5e-4\n","NUM_EPOCHS = 1000\n","MOMENTUM = 0.9\n","WEIGHT_DECAY = 0.99\n","MOMENTUM = 0.9"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-20T19:40:50.683950Z","iopub.status.busy":"2024-03-20T19:40:50.683491Z","iopub.status.idle":"2024-03-20T19:40:50.692584Z","shell.execute_reply":"2024-03-20T19:40:50.691221Z","shell.execute_reply.started":"2024-03-20T19:40:50.683917Z"},"trusted":true},"outputs":[],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-20T19:40:50.694479Z","iopub.status.busy":"2024-03-20T19:40:50.693893Z","iopub.status.idle":"2024-03-20T19:40:50.717065Z","shell.execute_reply":"2024-03-20T19:40:50.716034Z","shell.execute_reply.started":"2024-03-20T19:40:50.694417Z"},"trusted":true},"outputs":[],"source":["import torch.optim as optim\n","import torch.optim.lr_scheduler as lr_scheduler\n","\n","model = MLP()\n","\n","# checkpoint = torch.load(\"./checkpoints/checkpoint-0.0394-0.1206.pth\")\n","# model.load_state_dict(checkpoint['model_state_dict'])\n","\n","loss_fn = nn.MSELoss()\n","optimizer = optim.SGD(\n","    model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY, momentum=MOMENTUM\n",")\n","# scheduler = lr_scheduler.LinearLR(\n","#     optimizer, start_factor=1.0, end_factor=0.3, total_iters=NUM_EPOCHS * 0.5\n","# )\n","scheduler = lr_scheduler.CyclicLR(\n","    optimizer, base_lr=5e-4, max_lr=5e-3, step_size_up=100\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-20T19:40:50.718581Z","iopub.status.busy":"2024-03-20T19:40:50.718085Z","iopub.status.idle":"2024-03-20T19:40:50.729613Z","shell.execute_reply":"2024-03-20T19:40:50.728565Z","shell.execute_reply.started":"2024-03-20T19:40:50.718550Z"},"trusted":true},"outputs":[],"source":["from torch.utils.tensorboard import SummaryWriter\n","\n","def train(model, dataloader, optimizer, scheduler, loss_fn, epoch, writer, log_perc = 0.1):\n","    model.train()\n","    total_loss = 0\n","    total_diff = 0\n","\n","    logs_steps = int(log_perc * len(dataloader))\n","    start_step = epoch * len(dataloader)\n","    \n","    before_lr = optimizer.param_groups[0][\"lr\"]\n","    writer.add_scalar('Lr/Train', before_lr, epoch)\n","    for idx, (inputs, targets) in enumerate(dataloader):\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        \n","        optimizer.zero_grad()\n","\n","        outputs = model(inputs)\n","\n","        loss = loss_fn(outputs, targets)\n","        diff = torch.abs(outputs - targets).mean()\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","        total_diff += diff.item()\n","        \n","        if idx % logs_steps == 0:\n","            writer.add_scalar('Loss/Train', loss.item(), start_step + idx)\n","            writer.add_scalar('Absolute Difference/Train', diff.item(), start_step + idx)\n","            \n","            if SHOULD_PRINT:\n","                print(f\"Loss/Train: {loss.item()}\")\n","                print(f\"Absolute Difference/Train: {diff.item()}\")\n","\n","    scheduler.step()\n","    after_lr = optimizer.param_groups[0][\"lr\"]\n","    average_loss = total_loss / len(dataloader)\n","    average_diff = total_diff / len(dataloader)\n","    \n","    writer.add_scalar('Avg Loss/Train', average_loss, epoch)\n","    writer.add_scalar('Avg Absolute Difference/Train', average_diff, epoch)\n","    writer.add_scalar('Lr/Train', after_lr, epoch)\n","    \n","    if SHOULD_PRINT:\n","        print(f\"Avg Loss/Train: {average_loss}\")\n","        print(f\"Absolute Difference/Train: {average_diff}\")\n","        print(f\"Lr/Train: {after_lr}\")\n","\n","    # print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Train Loss: {average_loss:.4f}, Train Diff: {average_diff:.15f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-20T19:40:50.732028Z","iopub.status.busy":"2024-03-20T19:40:50.730930Z","iopub.status.idle":"2024-03-20T19:40:50.746779Z","shell.execute_reply":"2024-03-20T19:40:50.745210Z","shell.execute_reply.started":"2024-03-20T19:40:50.731979Z"},"trusted":true},"outputs":[],"source":["def validate(model, dataloader, loss_fn, epoch, writer):\n","    model.eval()\n","    total_loss = 0\n","    total_diff = 0\n","\n","    with torch.no_grad():\n","        for inputs, targets in dataloader:\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs = model(inputs)\n","\n","            loss = loss_fn(outputs, targets)\n","            diff = torch.abs(outputs - targets).mean()\n","            \n","            total_loss += loss.item()\n","            total_diff += diff.item()\n","\n","    average_loss = total_loss / len(dataloader)\n","    average_diff = total_diff / len(dataloader)\n","\n","    if writer is not None:\n","        writer.add_scalar('Avg Loss/Val', average_loss, epoch)\n","        writer.add_scalar('Avg Absolute Difference/Val', average_diff, epoch)\n","    \n","    if SHOULD_PRINT:\n","        print(f\"Avg Loss/Val: {average_loss}\")\n","        print(f\"Avg Absolute Difference/Val: {average_diff}\")\n","\n","    if epoch is not None:\n","        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Val Loss: {average_loss:.4f}, Val Diff: {average_diff:.15f}\")\n","    else:\n","        print(f\"Test Loss: {average_loss:.4f}, Test Diff: {average_diff:.15f}\")\n","        \n","    return average_diff"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-20T19:40:50.771845Z","iopub.status.busy":"2024-03-20T19:40:50.771483Z","iopub.status.idle":"2024-03-20T19:49:07.780059Z","shell.execute_reply":"2024-03-20T19:49:07.778660Z","shell.execute_reply.started":"2024-03-20T19:40:50.771812Z"},"trusted":true},"outputs":[],"source":["import datetime\n","\n","now = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","writer = SummaryWriter(f\"tb_logs/{now}\")\n","\n","model = model.to(device)\n","best_avg_diff = 1000\n","for epoch in range(NUM_EPOCHS):\n","    train(model, train_dataloader, optimizer, scheduler, loss_fn, epoch, writer)\n","\n","    average_diff = validate(model, val_dataloader, loss_fn, epoch, writer)\n","    if average_diff < best_avg_diff:\n","        best_avg_diff = average_diff\n","        torch.save(\n","            {\n","                \"epoch\": epoch,\n","                \"model_state_dict\": model.state_dict(),\n","                \"optimizer_state_dict\": optimizer.state_dict(),\n","                \"scheduler_state_dict\": scheduler.state_dict(),\n","                \"loss\": loss_fn,\n","            },\n","            \"checkpoint.pth\",\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","print(\"Best average diff:\", best_avg_diff)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-20T19:49:13.766513Z","iopub.status.busy":"2024-03-20T19:49:13.766147Z","iopub.status.idle":"2024-03-20T19:49:13.835425Z","shell.execute_reply":"2024-03-20T19:49:13.834548Z","shell.execute_reply.started":"2024-03-20T19:49:13.766485Z"},"trusted":true},"outputs":[],"source":["validate(model, test_dataloader, loss_fn, epoch, None)"]},{"cell_type":"markdown","metadata":{},"source":["## Testing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["checkpoint = torch.load(\"checkpoints\\checkpoint-0.0394-0.1206.pth\")\n","model = MLP()\n","model.load_state_dict(checkpoint['model_state_dict'])\n","model = model.to(device)\n","model.eval()\n","\n","validate(model, val_dataloader, loss_fn, None, None)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4578667,"sourceId":7815780,"sourceType":"datasetVersion"},{"datasetId":4638556,"sourceId":7898704,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
