{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.66.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting torch-summary\n",
      "  Downloading torch_summary-1.4.5-py3-none-any.whl.metadata (18 kB)\n",
      "Downloading torch_summary-1.4.5-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: torch-summary\n",
      "Successfully installed torch-summary-1.4.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.4.1.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.24.1)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.4.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.4.1.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.4.0-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.4.1.post1 scipy-1.13.0 threadpoolctl-3.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard)\n",
      "  Downloading grpcio-1.62.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard)\n",
      "  Downloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.24.1)\n",
      "Collecting protobuf!=4.24.0,>=3.19.6 (from tensorboard)\n",
      "  Downloading protobuf-5.26.1-cp37-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (68.2.2)\n",
      "Requirement already satisfied: six>1.9 in /usr/lib/python3/dist-packages (from tensorboard) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard)\n",
      "  Downloading werkzeug-3.0.2-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.2)\n",
      "Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.62.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.26.1-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.8/302.8 kB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.0.2-py3-none-any.whl (226 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.8/226.8 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: werkzeug, tensorboard-data-server, protobuf, markdown, grpcio, absl-py, tensorboard\n",
      "Successfully installed absl-py-2.1.0 grpcio-1.62.1 markdown-3.6 protobuf-5.26.1 tensorboard-2.16.2 tensorboard-data-server-0.7.2 werkzeug-3.0.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "!pip install torch-summary\n",
    "!pip install -U scikit-learn\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOULD_PRINT = True\n",
    "SEED = 32\n",
    "CONTINUE_MODEL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5e589d8a30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines_to_remove = set()\n",
    "# i = 0\n",
    "# with open(\"full_dataset.txt\", 'r', encoding='utf-8') as f:\n",
    "#     for line in f:\n",
    "#         i += 1\n",
    "#         numbers = [float(num) for num in line.strip().split()]\n",
    "#         if len(numbers) == 6:\n",
    "#             continue\n",
    "#         lines_to_remove.add(i)\n",
    "\n",
    "# i = 0\n",
    "# with open(\"full_dataset.txt\", 'r', encoding='utf-8') as fr:\n",
    "#     with open(\"full_dataset_2.txt\", 'w', encoding='utf-8') as fw:\n",
    "#         for line in fr:\n",
    "#             i += 1\n",
    "#             if i in lines_to_remove:\n",
    "#                 continue\n",
    "#             fw.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, large_file_path, chunk_size, subset_size=50000):\n",
    "        self.large_file_path = large_file_path\n",
    "        self.chunk_size = chunk_size\n",
    "        self.subset_size = subset_size\n",
    "        self.line_offsets = self.get_line_offsets(large_file_path, chunk_size)\n",
    "        self.scaler = StandardScaler()\n",
    "        print(\"Calculating mean and std...\")\n",
    "        self.mean, self.std = self.calculate_mean_std()\n",
    "        print(f\"Mean: {self.mean}, Std: {self.std}\")\n",
    "\n",
    "    def get_line_offsets(self, path: str, chunk_size: int) -> List[int]:\n",
    "        offsets = [0]\n",
    "        with open(path, \"rb\") as file:\n",
    "            chunk = file.readlines(chunk_size)\n",
    "            while chunk:\n",
    "                for line in chunk:\n",
    "                    offsets.append(offsets[-1] + len(line))\n",
    "                chunk = file.readlines(chunk_size)\n",
    "                print(f\"Lines found: {len(offsets)}\", end='\\r')\n",
    "        offsets = offsets[:-1]\n",
    "        print(f\"Lines found: {len(offsets)}\", end='\\n')\n",
    "        return offsets\n",
    "\n",
    "    def calculate_mean_std(self):\n",
    "        selected_offsets = random.sample(self.line_offsets, min(self.subset_size, len(self.line_offsets)))\n",
    "        features = []\n",
    "        for offset in selected_offsets:\n",
    "            with open(self.large_file_path, 'r', encoding='utf-8') as f:\n",
    "                f.seek(offset)\n",
    "                line = f.readline()\n",
    "                numbers = [float(num) for num in line.strip().split()]\n",
    "                features.append(numbers[:4])\n",
    "        features = np.array(features)\n",
    "        mean = np.mean(features, axis=0, dtype=np.float32)\n",
    "        std = np.std(features, axis=0, dtype=np.float32)\n",
    "        return mean, std\n",
    "\n",
    "    def standardize_features(self, features):\n",
    "        standardized_features = (features - self.mean) / self.std\n",
    "        return standardized_features\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.line_offsets)\n",
    "\n",
    "    def __getitem__(self, line):\n",
    "        offset = self.line_offsets[line]\n",
    "        with open(self.large_file_path, 'r', encoding='utf-8') as f:\n",
    "            f.seek(offset)\n",
    "            line = f.readline()\n",
    "            numbers = [float(num) for num in line.strip().split()]\n",
    "            features, targets = numbers[:4], numbers[4:]\n",
    "            standardized_features = self.standardize_features(np.array(features))\n",
    "            return torch.tensor(standardized_features, dtype=torch.float32), torch.tensor(targets, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines found: 7154565\n",
      "Calculating mean and std...\n",
      "Mean: [24.4646    -0.08918    1.566072   3.1192112], Std: [ 8.830875  14.969308   0.9238345  1.8094094]\n"
     ]
    }
   ],
   "source": [
    "filename = \"full_dataset_2.txt\"\n",
    "full_dataset = CustomDataset(filename, 2**20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * len(full_dataset))\n",
    "rest_size = len(full_dataset) - train_size\n",
    "val_size = rest_size // 10\n",
    "test_size = rest_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 5008195\n",
      "Validation size: 214637\n",
      "Test size: 1931733\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train size: {train_size}\")\n",
    "print(f\"Validation size: {val_size}\")\n",
    "print(f\"Test size: {test_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 2 ** 19\n",
    "train_shuffle = True\n",
    "val_shuffle = False\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=train_shuffle)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=val_shuffle)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_layers, out_layers):\n",
    "        super(Block, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_layers, out_layers),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout1d(p=0.2),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(4, 512),\n",
    "            Block(512, 512),\n",
    "            Block(512, 512),\n",
    "        ])\n",
    "        self.bottleneck = nn.ModuleList([\n",
    "            Block(512, 512),\n",
    "            nn.Linear(512, 4),\n",
    "        ])\n",
    "        \n",
    "        self.__init_weights()\n",
    "\n",
    "    def __init_weights(self):\n",
    "        layers = [self.blocks, self.bottleneck]\n",
    "        # Initialize linear layers using Kaiming (He) uniform initialization\n",
    "        for m in layers:\n",
    "            for layer in m:\n",
    "                self.__init_layer(layer)\n",
    "                        \n",
    "    def __init_layer(self, layer):\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            init.kaiming_uniform_(layer.weight, mode='fan_in', nonlinearity='tanh')\n",
    "            if layer.bias is not None:\n",
    "                init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.blocks[0](x)\n",
    "        for block in self.blocks[1:]:\n",
    "            y = block(x)\n",
    "            x = x + y\n",
    "        for btl in self.bottleneck:\n",
    "            x = btl(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(4, 512),\n",
    "            Block(512, 512),\n",
    "            Block(512, 512),\n",
    "            Block(512, 512),\n",
    "        ])\n",
    "        self.out = nn.Linear(512, 2)\n",
    "        self.__init_weights()\n",
    "\n",
    "    def __init_weights(self):\n",
    "        layers = [self.blocks]\n",
    "        # Initialize linear layers using Kaiming (He) uniform initialization\n",
    "        for m in layers:\n",
    "            for layer in m:\n",
    "                self.__init_layer(layer)\n",
    "        self.__init_layer(self.out)\n",
    "                        \n",
    "    def __init_layer(self, layer):\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            init.kaiming_uniform_(layer.weight, mode='fan_in', nonlinearity='tanh')\n",
    "            if layer.bias is not None:\n",
    "                init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.blocks[0](x)\n",
    "        for block in self.blocks[1:]:\n",
    "            y = block(x)\n",
    "            x = x + y\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionAutoencoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, freeze):\n",
    "        super(RegressionAutoencoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.gradient(self.encoder, freeze)\n",
    "        \n",
    "    def gradient(self, model, freeze: bool):\n",
    "        for parameter in model.parameters():\n",
    "            parameter.requires_grad_(not freeze)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def train(is_encoder, model, dataloader, optimizer, scheduler, loss_fn, epoch, writer, log_perc = 0.1):\n",
    "    model.train()\n",
    "    model = model.to(DEVICE)\n",
    "    total_loss = 0\n",
    "    total_diff = 0\n",
    "    best_diff = 100\n",
    "\n",
    "    logs_steps = max(int(log_perc * len(dataloader)), 1)\n",
    "    start_step = epoch * len(dataloader)\n",
    "\n",
    "    before_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    writer.add_scalar('Lr/Train', before_lr, epoch)\n",
    "    for idx, (inputs, targets) in enumerate(tqdm(dataloader)):\n",
    "        inputs, targets = inputs.to(DEVICE).to(torch.float32), targets.to(DEVICE).to(torch.float32)\n",
    "        \n",
    "        if is_encoder:\n",
    "            targets = inputs\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        diff = torch.abs(outputs - targets).mean()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_diff += diff.item()\n",
    "        \n",
    "        if idx % logs_steps == 0:\n",
    "            writer.add_scalar('Loss/Train', loss.item(), start_step + idx)\n",
    "            writer.add_scalar('Absolute Difference/Train', diff.item(), start_step + idx)\n",
    "            \n",
    "            if SHOULD_PRINT:\n",
    "                print(f\"Loss/Train: {loss.item()}\")\n",
    "                print(f\"Absolute Difference/Train: {diff.item()}\")\n",
    "        \n",
    "    after_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    average_diff = total_diff / len(dataloader)\n",
    "    \n",
    "    writer.add_scalar('Avg Loss/Train', average_loss, epoch)\n",
    "    writer.add_scalar('Avg Absolute Difference/Train', average_diff, epoch)\n",
    "    writer.add_scalar('Lr/Train', after_lr, epoch)\n",
    "    \n",
    "    if SHOULD_PRINT:\n",
    "        print(f\"Avg Loss/Train: {average_loss}\")\n",
    "        print(f\"Avg Absolute Difference/Train: {average_diff}\")\n",
    "        print(f\"Lr/Train: {after_lr}\")\n",
    "\n",
    "    # print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Train Loss: {average_loss:.4f}, Train Diff: {average_diff:.15f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(is_encoder, model, dataloader, loss_fn, epoch, writer):\n",
    "    model.eval()\n",
    "    model = model.to(DEVICE)\n",
    "    total_loss = 0\n",
    "    total_diff = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(DEVICE).to(torch.float32), targets.to(DEVICE).to(torch.float32)\n",
    "            \n",
    "            if is_encoder:\n",
    "                targets = inputs\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            diff = torch.abs(outputs - targets).mean()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_diff += diff.item()\n",
    "\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    average_diff = total_diff / len(dataloader)\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.add_scalar('Avg Loss/Val', average_loss, epoch)\n",
    "        writer.add_scalar('Avg Absolute Difference/Val', average_diff, epoch)\n",
    "    \n",
    "    if SHOULD_PRINT:\n",
    "        print(f\"Avg Loss/Val: {average_loss}\")\n",
    "        print(f\"Avg Absolute Difference/Val: {average_diff}\")\n",
    "\n",
    "    if epoch is not None:\n",
    "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Val Loss: {average_loss:.4f}, Val Diff: {average_diff:.15f}\")\n",
    "    else:\n",
    "        print(f\"Test Loss: {average_loss:.4f}, Test Diff: {average_diff:.15f}\")\n",
    "        \n",
    "    return average_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "NUM_EPOCHS = 40\n",
    "WEIGHT_DECAY = 0.99\n",
    "WEIGHT_DECAY_L1 = 1e-4\n",
    "MOMENTUM = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000: adjusting learning rate of group 0 to 1.0000e-03.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "last_epoch = 0\n",
    "encoder_model = Encoder()\n",
    "loss_mse = nn.MSELoss()\n",
    "optimizer = optim.SGD(encoder_model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY, nesterov=True)\n",
    "scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, verbose=True)\n",
    "if CONTINUE_MODEL:\n",
    "    checkpoint = torch.load(\"encoder_checkpoint.pth\")\n",
    "    encoder_model.load_state_dict(checkpoint['encoder_model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    last_epoch = checkpoint['epoch']\n",
    "    loss_mse = checkpoint['loss_mse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_loss(outputs, targets):\n",
    "    loss = loss_mse(outputs, targets)\n",
    "#     loss_rmse = torch.sqrt(loss)\n",
    "#     l1_reg = torch.tensor(0., requires_grad=True)\n",
    "\n",
    "#     for name, param in encoder_model.named_parameters():\n",
    "#         if 'weight' in name:\n",
    "#             l1_reg = l1_reg + torch.linalg.norm(param, 1)\n",
    "\n",
    "#     total_loss = (loss * 0.8 + loss_rmse * 0.2) + WEIGHT_DECAY_L1 * l1_reg\n",
    "#     total_loss = (loss * 0.8 + loss_rmse * 0.2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datetime\n",
    "\n",
    "# now = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# writer = SummaryWriter(f\"tb_logs/encoder/{now}\")\n",
    "\n",
    "# VALIDATION_STEPS = 6\n",
    "\n",
    "# encoder_model = encoder_model.to(DEVICE)\n",
    "# best_avg_diff = 1000\n",
    "\n",
    "# for idx, epoch in enumerate(range(last_epoch, NUM_EPOCHS)):\n",
    "#     train(True, encoder_model, train_dataloader, optimizer, scheduler, total_loss, epoch, writer, log_perc=0.2)\n",
    "\n",
    "#     if idx % VALIDATION_STEPS == 0:\n",
    "#         average_diff = validate(True, encoder_model, val_dataloader, total_loss, epoch, writer)\n",
    "#         if average_diff < best_avg_diff:\n",
    "#             torch.save({\n",
    "#                 'epoch': epoch,\n",
    "#                 'encoder_model_state_dict': encoder_model.state_dict(),\n",
    "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                 'scheduler_state_dict': scheduler.state_dict(),\n",
    "#                 'loss_mse': loss_mse,\n",
    "#                 }, \"encoder_checkpoint.pth\")\n",
    "\n",
    "# # Launch TensorBoard: `tensorboard --logdir=tb_logs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validate(True, encoder_model, test_dataloader, total_loss, epoch, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 5e-3\n",
    "NUM_EPOCHS = 40\n",
    "WEIGHT_DECAY = 0.99\n",
    "WEIGHT_DECAY_L1 = 1e-4\n",
    "MOMENTUM = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000: adjusting learning rate of group 0 to 5.0000e-03.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "last_epoch = 0\n",
    "decoder = Decoder()\n",
    "autoencoder_model = RegressionAutoencoder(encoder=encoder_model, decoder=decoder, freeze=True)\n",
    "autoencoder_model = autoencoder_model.to(DEVICE)\n",
    "\n",
    "loss_mse = nn.MSELoss()\n",
    "optimizer = optim.SGD(autoencoder_model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY, nesterov=True)\n",
    "scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, verbose=True)\n",
    "if CONTINUE_MODEL:\n",
    "    checkpoint = torch.load(\"regressor_freezed_checkpoint.pth\")\n",
    "    autoencoder_model.load_state_dict(checkpoint['autoencoder_model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    last_epoch = checkpoint['epoch']\n",
    "    loss_mse = checkpoint['loss_mse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_loss(outputs, targets):\n",
    "    loss = loss_mse(outputs, targets)\n",
    "#     loss_rmse = torch.sqrt(loss)\n",
    "#     l1_reg = torch.tensor(0., requires_grad=True)\n",
    "\n",
    "#     for name, param in autoencoder_model.named_parameters():\n",
    "#         if 'weight' in name:\n",
    "#             l1_reg = l1_reg + torch.linalg.norm(param, 1)\n",
    "\n",
    "#     total_loss = (loss * 0.8 + loss_rmse * 0.2) + WEIGHT_DECAY_L1 * l1_reg\n",
    "#     total_loss = (loss * 0.8 + loss_rmse * 0.2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datetime\n",
    "\n",
    "# now = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# writer = SummaryWriter(f\"tb_logs/regressor/{now}\")\n",
    "\n",
    "# VALIDATION_STEPS = 6\n",
    "\n",
    "# best_avg_diff = 1000\n",
    "\n",
    "# for idx, epoch in enumerate(range(last_epoch, NUM_EPOCHS)):\n",
    "#     train(False, autoencoder_model, train_dataloader, optimizer, scheduler, total_loss, epoch, writer, log_perc=0.2)\n",
    "\n",
    "#     if idx % VALIDATION_STEPS == 0:\n",
    "#         average_diff = validate(False, autoencoder_model, val_dataloader, total_loss, epoch, writer)\n",
    "#         if average_diff < best_avg_diff:\n",
    "#             torch.save({\n",
    "#                 'epoch': epoch,\n",
    "#                 'autoencoder_model_state_dict': autoencoder_model.state_dict(),\n",
    "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                 'scheduler_state_dict': scheduler.state_dict(),\n",
    "#                 'loss_mse': loss_mse,\n",
    "#                 }, \"regressor_freezed_checkpoint.pth\")\n",
    "\n",
    "# # Launch TensorBoard: `tensorboard --logdir=tb_logs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validate(False, autoencoder_model, test_dataloader, total_loss, epoch, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unfreezed Regression Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-4\n",
    "NUM_EPOCHS = 60\n",
    "WEIGHT_DECAY = 0.99\n",
    "WEIGHT_DECAY_L1 = 1e-4\n",
    "MOMENTUM = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000: adjusting learning rate of group 0 to 5.0000e-04.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "last_epoch = 0\n",
    "decoder = Decoder()\n",
    "autoencoder_model = RegressionAutoencoder(encoder=encoder_model, decoder=decoder, freeze=False)\n",
    "autoencoder_model = autoencoder_model.to(DEVICE)\n",
    "\n",
    "loss_mse = nn.MSELoss()\n",
    "optimizer = optim.SGD(autoencoder_model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY, nesterov=True)\n",
    "scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, verbose=True)\n",
    "# if CONTINUE_MODEL:\n",
    "#     checkpoint = torch.load(\"regressor_unfreezed_checkpoint.pth\")\n",
    "#     autoencoder_model.load_state_dict(checkpoint['autoencoder_model_state_dict'])\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#     scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "#     last_epoch = checkpoint['epoch']\n",
    "#     loss_mse = checkpoint['loss_mse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_loss(outputs, targets):\n",
    "    loss = loss_mse(outputs, targets)\n",
    "#     loss_rmse = torch.sqrt(loss)\n",
    "#     l1_reg = torch.tensor(0., requires_grad=True)\n",
    "\n",
    "#     for name, param in autoencoder_model.named_parameters():\n",
    "#         if 'weight' in name:\n",
    "#             l1_reg = l1_reg + torch.linalg.norm(param, 1)\n",
    "\n",
    "#     total_loss = (loss * 0.8 + loss_rmse * 0.2) + WEIGHT_DECAY_L1 * l1_reg\n",
    "#     total_loss = (loss * 0.8 + loss_rmse * 0.2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:20<03:03, 20.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss/Train: 0.040549829602241516\n",
      "Absolute Difference/Train: 0.1321011483669281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [01:00<02:21, 20.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss/Train: 0.04028385505080223\n",
      "Absolute Difference/Train: 0.13176323473453522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [01:42<01:42, 20.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss/Train: 0.04018343985080719\n",
      "Absolute Difference/Train: 0.13150900602340698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [02:22<01:00, 20.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss/Train: 0.04037809371948242\n",
      "Absolute Difference/Train: 0.1318116933107376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [03:03<00:20, 20.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss/Train: 0.040521085262298584\n",
      "Absolute Difference/Train: 0.13187266886234283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:15<00:00, 19.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss/Train: 0.04034961797297001\n",
      "Avg Absolute Difference/Train: 0.13175572454929352\n",
      "Lr/Train: 0.0005\n",
      "Avg Loss/Val: 0.039884015917778015\n",
      "Avg Absolute Difference/Val: 0.12896744906902313\n",
      "Epoch [1/60] Val Loss: 0.0399, Val Diff: 0.128967449069023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:22<03:26, 22.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss/Train: 0.04049462825059891\n",
      "Absolute Difference/Train: 0.13175496459007263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [01:03<02:25, 20.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss/Train: 0.04042419046163559\n",
      "Absolute Difference/Train: 0.13164778053760529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [01:45<01:45, 21.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss/Train: 0.04019670933485031\n",
      "Absolute Difference/Train: 0.13116811215877533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [02:29<01:05, 21.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss/Train: 0.04024902731180191\n",
      "Absolute Difference/Train: 0.131111741065979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [03:12<00:21, 21.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss/Train: 0.04033805802464485\n",
      "Absolute Difference/Train: 0.13117845356464386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:23<00:00, 20.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss/Train: 0.04028437770903111\n",
      "Avg Absolute Difference/Train: 0.1312222272157669\n",
      "Lr/Train: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:20<03:01, 20.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss/Train: 0.04021774232387543\n",
      "Absolute Difference/Train: 0.13076844811439514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [01:00<02:22, 20.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss/Train: 0.03998686745762825\n",
      "Absolute Difference/Train: 0.13073599338531494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [01:41<01:41, 20.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss/Train: 0.04044201970100403\n",
      "Absolute Difference/Train: 0.13113151490688324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [02:23<01:02, 20.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss/Train: 0.04003289341926575\n",
      "Absolute Difference/Train: 0.13073474168777466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [03:04<00:20, 20.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss/Train: 0.04032101109623909\n",
      "Absolute Difference/Train: 0.13073238730430603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:16<00:00, 19.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss/Train: 0.04025083482265472\n",
      "Avg Absolute Difference/Train: 0.13085227608680725\n",
      "Lr/Train: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:21<03:17, 21.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss/Train: 0.040284231305122375\n",
      "Absolute Difference/Train: 0.13077348470687866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [01:03<02:28, 21.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss/Train: 0.040145810693502426\n",
      "Absolute Difference/Train: 0.13043947517871857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [01:43<01:42, 20.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss/Train: 0.040174826979637146\n",
      "Absolute Difference/Train: 0.13049714267253876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [02:25<01:02, 20.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss/Train: 0.04007745534181595\n",
      "Absolute Difference/Train: 0.13030892610549927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [02:46<00:41, 20.71s/it]"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "writer = SummaryWriter(f\"tb_logs/regressor/{now}\")\n",
    "\n",
    "VALIDATION_STEPS = 6\n",
    "\n",
    "best_avg_diff = 1000\n",
    "\n",
    "for idx, epoch in enumerate(range(last_epoch, NUM_EPOCHS)):\n",
    "    # train(False, autoencoder_model, train_dataloader, optimizer, scheduler, total_loss, epoch, writer, log_perc=0.2)\n",
    "    train(False, autoencoder_model, train_dataloader, optimizer, None, total_loss, epoch, writer, log_perc=0.2)\n",
    "\n",
    "    if idx % VALIDATION_STEPS == 0:\n",
    "        average_diff = validate(False, autoencoder_model, val_dataloader, total_loss, epoch, writer)\n",
    "        if average_diff < best_avg_diff:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'autoencoder_model_state_dict': autoencoder_model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'loss_mse': loss_mse,\n",
    "                }, \"regressor_unfreezed_checkpoint.pth\")\n",
    "\n",
    "# Launch TensorBoard: `tensorboard --logdir=tb_logs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validate(False, autoencoder_model, test_dataloader, total_loss, epoch, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4578667,
     "sourceId": 7960124,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
