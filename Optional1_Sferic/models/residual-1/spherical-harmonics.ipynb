{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torch-summary in /usr/local/lib/python3.10/dist-packages (1.4.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.4.1.post1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.24.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard)\n",
      "  Downloading grpcio-1.62.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard)\n",
      "  Downloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.24.1)\n",
      "Collecting protobuf!=4.24.0,>=3.19.6 (from tensorboard)\n",
      "  Downloading protobuf-5.26.1-cp37-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (68.2.2)\n",
      "Requirement already satisfied: six>1.9 in /usr/lib/python3/dist-packages (from tensorboard) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard)\n",
      "  Downloading werkzeug-3.0.2-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.2)\n",
      "Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.62.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.26.1-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.8/302.8 kB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.0.2-py3-none-any.whl (226 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.8/226.8 kB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: werkzeug, tensorboard-data-server, protobuf, markdown, grpcio, absl-py, tensorboard\n",
      "Successfully installed absl-py-2.1.0 grpcio-1.62.1 markdown-3.6 protobuf-5.26.1 tensorboard-2.16.2 tensorboard-data-server-0.7.2 werkzeug-3.0.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "!pip install torch-summary\n",
    "!pip install -U scikit-learn\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOULD_PRINT = True\n",
    "SEED = 32\n",
    "CONTINUE_MODEL = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff6f01dc710>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset\n",
    "\n",
    "# class CustomDataset(Dataset):\n",
    "#     def __init__(self, filename):\n",
    "#         self.data = []\n",
    "#         with open(filename, 'r') as f:\n",
    "#             for line in f:\n",
    "#                 numbers = [float(num) for num in line.strip().split()]\n",
    "#                 self.data.append((numbers[:4], numbers[4:]))\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         inputs, targets = self.data[idx]\n",
    "#         return torch.tensor(inputs), torch.tensor(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# from typing import List\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "# class CustomDataset(Dataset):\n",
    "#     def __init__(self, large_file_path, chunk_size):\n",
    "#         self.large_file_path = large_file_path\n",
    "#         self.line_offsets = self.get_line_offsets(large_file_path, chunk_size)\n",
    "\n",
    "#     def get_line_offsets(self, path: str, chunk_size: int) -> List[int]:\n",
    "#         offsets = [0]\n",
    "#         with open(path, \"rb\") as file:\n",
    "#             chunk = file.readlines(chunk_size)\n",
    "#             while chunk:\n",
    "#                 for line in chunk:\n",
    "#                     offsets.append(offsets[-1] + len(line))\n",
    "#                 chunk = file.readlines(chunk_size)\n",
    "#                 print(f\"Lines found: {len(offsets)}\", end='\\r')\n",
    "#         offsets = offsets[:-1]\n",
    "#         return offsets\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.line_offsets)\n",
    "    \n",
    "#     def __getitem__(self, line):\n",
    "#         offset = self.line_offsets[line]\n",
    "#         with open(self.large_file_path, 'r', encoding='utf-8') as f:\n",
    "#             f.seek(offset)\n",
    "#             line = f.readline()\n",
    "#             numbers = [float(num) for num in line.strip().split()]\n",
    "#             inputs, targets = numbers[:4], numbers[4:]\n",
    "#             return torch.tensor(inputs), torch.tensor(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.5, 2.5, 3.5, 4.5])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "features = [[1, 2, 3, 4], [2, 3, 4, 5]]\n",
    "mean = np.mean(features, axis=0)\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, large_file_path, chunk_size, subset_size=50000):\n",
    "        self.large_file_path = large_file_path\n",
    "        self.chunk_size = chunk_size\n",
    "        self.subset_size = subset_size\n",
    "        self.line_offsets = self.get_line_offsets(large_file_path, chunk_size)\n",
    "        self.scaler = StandardScaler()\n",
    "        print(\"Calculating mean and std...\")\n",
    "        self.mean, self.std = self.calculate_mean_std()\n",
    "        print(f\"Mean: {self.mean}, Std: {self.std}\")\n",
    "\n",
    "    def get_line_offsets(self, path: str, chunk_size: int) -> List[int]:\n",
    "        offsets = [0]\n",
    "        with open(path, \"rb\") as file:\n",
    "            chunk = file.readlines(chunk_size)\n",
    "            while chunk:\n",
    "                for line in chunk:\n",
    "                    offsets.append(offsets[-1] + len(line))\n",
    "                chunk = file.readlines(chunk_size)\n",
    "                print(f\"Lines found: {len(offsets)}\", end='\\r')\n",
    "        offsets = offsets[:-1]\n",
    "        print(f\"Lines found: {len(offsets)}\", end='\\n')\n",
    "        return offsets\n",
    "\n",
    "    def calculate_mean_std(self):\n",
    "        selected_offsets = random.sample(self.line_offsets, min(self.subset_size, len(self.line_offsets)))\n",
    "        features = []\n",
    "        for offset in selected_offsets:\n",
    "            with open(self.large_file_path, 'r', encoding='utf-8') as f:\n",
    "                f.seek(offset)\n",
    "                line = f.readline()\n",
    "                numbers = [float(num) for num in line.strip().split()]\n",
    "                features.append(numbers[:4])\n",
    "        features = np.array(features)\n",
    "        mean = np.mean(features, axis=0, dtype=np.float32)\n",
    "        std = np.std(features, axis=0, dtype=np.float32)\n",
    "        return mean, std\n",
    "\n",
    "    def standardize_features(self, features):\n",
    "        standardized_features = (features - self.mean) / self.std\n",
    "        return standardized_features\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.line_offsets)\n",
    "\n",
    "    def __getitem__(self, line):\n",
    "        offset = self.line_offsets[line]\n",
    "        with open(self.large_file_path, 'r', encoding='utf-8') as f:\n",
    "            f.seek(offset)\n",
    "            line = f.readline()\n",
    "            numbers = [float(num) for num in line.strip().split()]\n",
    "            features, targets = numbers[:4], numbers[4:]\n",
    "            standardized_features = self.standardize_features(np.array(features))\n",
    "            return torch.tensor(standardized_features, dtype=torch.float32), torch.tensor(targets, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines found: 6367855\n",
      "Calculating mean and std...\n",
      "Mean: [ 2.3069660e+01 -4.5200000e-03  1.5650475e+00  3.1191590e+00], Std: [ 8.339648   14.216207    0.92338675  1.8202546 ]\n"
     ]
    }
   ],
   "source": [
    "filename = \"full_dataset.txt\"\n",
    "full_dataset = CustomDataset(filename, 2**20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.1 * len(full_dataset))\n",
    "rest_size = len(full_dataset) - train_size\n",
    "val_size = rest_size // 10\n",
    "test_size = rest_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "636785"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 2 ** 20\n",
    "train_shuffle = True\n",
    "val_shuffle = False\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=train_shuffle)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=val_shuffle)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for inputs, targets in train_dataloader:\n",
    "#     assert inputs.shape[1] == 4 and targets.shape[1] == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_layers, out_layers):\n",
    "        super(Block, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_layers, out_layers),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout1d(p=0.1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(4, 16),\n",
    "            Block(16, 16),\n",
    "            Block(16, 16),\n",
    "        ])\n",
    "        self.out = nn.Linear(16, 2)\n",
    "        \n",
    "        self.__init_weights()\n",
    "\n",
    "    def __init_weights(self):\n",
    "        layers = [self.blocks]\n",
    "        # Initialize linear layers using Kaiming (He) uniform initialization\n",
    "        for m in layers:\n",
    "            for layer in m:\n",
    "                self.__init_layer(layer)\n",
    "        self.__init_layer(self.out)\n",
    "                        \n",
    "    def __init_layer(self, layer):\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            init.kaiming_uniform_(layer.weight, mode='fan_in', nonlinearity='tanh')\n",
    "            if layer.bias is not None:\n",
    "                init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.blocks[0](x)\n",
    "        for block in self.blocks[1:]:\n",
    "            y = block(x)\n",
    "            x = x + y\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "NUM_EPOCHS = 12\n",
    "WEIGHT_DECAY = 0.99\n",
    "WEIGHT_DECAY_L1 = 1e-4\n",
    "MOMENTUM = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000: adjusting learning rate of group 0 to 1.0000e-03.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "last_epoch = 0\n",
    "model = MLP()\n",
    "loss_mse = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY, nesterov=True)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "# scheduler = lr_scheduler.LinearLR(\n",
    "#     optimizer,\n",
    "#     start_factor=1.0,\n",
    "#     end_factor=0.2,\n",
    "#     total_iters=NUM_EPOCHS * 0.75)\n",
    "scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=300, verbose=True)\n",
    "if CONTINUE_MODEL:\n",
    "    checkpoint = torch.load(\"checkpoint.pth\")\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    last_epoch = checkpoint['epoch']\n",
    "    loss_mse = checkpoint['loss_mse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "\n",
    "# summary(model, (BATCH_SIZE, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def train(model, dataloader, optimizer, scheduler, loss_fn, epoch, writer, log_perc = 0.1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_diff = 0\n",
    "    best_diff = 100\n",
    "\n",
    "    logs_steps = max(int(log_perc * len(dataloader)), 1)\n",
    "    start_step = epoch * len(dataloader)\n",
    "\n",
    "    before_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    writer.add_scalar('Lr/Train', before_lr, epoch)\n",
    "    for idx, (inputs, targets) in enumerate(tqdm(dataloader)):\n",
    "        inputs, targets = inputs.to(device).to(torch.float32), targets.to(device).to(torch.float32)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        diff = torch.abs(outputs - targets).mean()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_diff += diff.item()\n",
    "        \n",
    "        if idx % logs_steps == 0:\n",
    "            writer.add_scalar('Loss/Train', loss.item(), start_step + idx)\n",
    "            writer.add_scalar('Absolute Difference/Train', diff.item(), start_step + idx)\n",
    "            \n",
    "            if SHOULD_PRINT:\n",
    "                print(f\"Loss/Train: {loss.item()}\")\n",
    "                print(f\"Absolute Difference/Train: {diff.item()}\")\n",
    "        \n",
    "    after_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    average_diff = total_diff / len(dataloader)\n",
    "    \n",
    "    writer.add_scalar('Avg Loss/Train', average_loss, epoch)\n",
    "    writer.add_scalar('Avg Absolute Difference/Train', average_diff, epoch)\n",
    "    writer.add_scalar('Lr/Train', after_lr, epoch)\n",
    "    \n",
    "    if SHOULD_PRINT:\n",
    "        print(f\"Avg Loss/Train: {average_loss}\")\n",
    "        print(f\"Avg Absolute Difference/Train: {average_diff}\")\n",
    "        print(f\"Lr/Train: {after_lr}\")\n",
    "\n",
    "    # print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Train Loss: {average_loss:.4f}, Train Diff: {average_diff:.15f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, loss_fn, epoch, writer):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_diff = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device).to(torch.float32), targets.to(device).to(torch.float32)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            diff = torch.abs(outputs - targets).mean()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_diff += diff.item()\n",
    "\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    average_diff = total_diff / len(dataloader)\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.add_scalar('Avg Loss/Val', average_loss, epoch)\n",
    "        writer.add_scalar('Avg Absolute Difference/Val', average_diff, epoch)\n",
    "    \n",
    "    if SHOULD_PRINT:\n",
    "        print(f\"Avg Loss/Val: {average_loss}\")\n",
    "        print(f\"Avg Absolute Difference/Val: {average_diff}\")\n",
    "\n",
    "    if epoch is not None:\n",
    "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Val Loss: {average_loss:.4f}, Val Diff: {average_diff:.15f}\")\n",
    "    else:\n",
    "        print(f\"Test Loss: {average_loss:.4f}, Test Diff: {average_diff:.15f}\")\n",
    "        \n",
    "    return average_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_loss(outputs, targets):\n",
    "    loss = loss_mse(outputs, targets)\n",
    "#     loss_rmse = torch.sqrt(loss)\n",
    "#     l1_reg = torch.tensor(0., requires_grad=True)\n",
    "\n",
    "#     for name, param in model.named_parameters():\n",
    "#         if 'weight' in name:\n",
    "#             l1_reg = l1_reg + torch.linalg.norm(param, 1)\n",
    "\n",
    "#     total_loss = (loss * 0.8 + loss_rmse * 0.2) + WEIGHT_DECAY_L1 * l1_reg\n",
    "#     total_loss = (loss * 0.8 + loss_rmse * 0.2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:38<00:00, 38.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00001: adjusting learning rate of group 0 to 9.9997e-04.\n",
      "Loss/Train: 1.1721032857894897\n",
      "Absolute Difference/Train: 0.8564786911010742\n",
      "Avg Loss/Train: 1.1721032857894897\n",
      "Avg Absolute Difference/Train: 0.8564786911010742\n",
      "Lr/Train: 0.000999972584682756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss/Val: 0.9965173602104187\n",
      "Avg Absolute Difference/Val: 0.8104553818702698\n",
      "Epoch [1/12] Val Loss: 0.9965, Val Diff: 0.810455381870270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:35<00:00, 35.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00002: adjusting learning rate of group 0 to 9.9989e-04.\n",
      "Loss/Train: 1.110743522644043\n",
      "Absolute Difference/Train: 0.8327600955963135\n",
      "Avg Loss/Train: 1.110743522644043\n",
      "Avg Absolute Difference/Train: 0.8327600955963135\n",
      "Lr/Train: 0.0009998903417374227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:36<00:00, 36.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00003: adjusting learning rate of group 0 to 9.9975e-04.\n",
      "Loss/Train: 1.0314662456512451\n",
      "Absolute Difference/Train: 0.8009620904922485\n",
      "Avg Loss/Train: 1.0314662456512451\n",
      "Avg Absolute Difference/Train: 0.8009620904922485\n",
      "Lr/Train: 0.0009997532801828658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:36<00:00, 36.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00004: adjusting learning rate of group 0 to 9.9956e-04.\n",
      "Loss/Train: 0.9402877688407898\n",
      "Absolute Difference/Train: 0.7632606029510498\n",
      "Avg Loss/Train: 0.9402877688407898\n",
      "Avg Absolute Difference/Train: 0.7632606029510498\n",
      "Lr/Train: 0.0009995614150494292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss/Val: 0.7535747289657593\n",
      "Avg Absolute Difference/Val: 0.6997882723808289\n",
      "Epoch [4/12] Val Loss: 0.7536, Val Diff: 0.699788272380829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:36<00:00, 36.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00005: adjusting learning rate of group 0 to 9.9931e-04.\n",
      "Loss/Train: 0.842823326587677\n",
      "Absolute Difference/Train: 0.7212889790534973\n",
      "Avg Loss/Train: 0.842823326587677\n",
      "Avg Absolute Difference/Train: 0.7212889790534973\n",
      "Lr/Train: 0.0009993147673772868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:36<00:00, 36.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: adjusting learning rate of group 0 to 9.9901e-04.\n",
      "Loss/Train: 0.7461751699447632\n",
      "Absolute Difference/Train: 0.6773069500923157\n",
      "Avg Loss/Train: 0.7461751699447632\n",
      "Avg Absolute Difference/Train: 0.6773069500923157\n",
      "Lr/Train: 0.0009990133642141358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:36<00:00, 36.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: adjusting learning rate of group 0 to 9.9866e-04.\n",
      "Loss/Train: 0.6541696786880493\n",
      "Absolute Difference/Train: 0.6331635117530823\n",
      "Avg Loss/Train: 0.6541696786880493\n",
      "Avg Absolute Difference/Train: 0.6331635117530823\n",
      "Lr/Train: 0.000998657238612229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss/Val: 0.5049132108688354\n",
      "Avg Absolute Difference/Val: 0.5691218376159668\n",
      "Epoch [7/12] Val Loss: 0.5049, Val Diff: 0.569121837615967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:36<00:00, 36.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00008: adjusting learning rate of group 0 to 9.9825e-04.\n",
      "Loss/Train: 0.5688587427139282\n",
      "Absolute Difference/Train: 0.5895238518714905\n",
      "Avg Loss/Train: 0.5688587427139282\n",
      "Avg Absolute Difference/Train: 0.5895238518714905\n",
      "Lr/Train: 0.0009982464296247522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:37<00:00, 37.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00009: adjusting learning rate of group 0 to 9.9778e-04.\n",
      "Loss/Train: 0.49190643429756165\n",
      "Absolute Difference/Train: 0.547673761844635\n",
      "Avg Loss/Train: 0.49190643429756165\n",
      "Avg Absolute Difference/Train: 0.547673761844635\n",
      "Lr/Train: 0.00099778098230154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:36<00:00, 36.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00010: adjusting learning rate of group 0 to 9.9726e-04.\n",
      "Loss/Train: 0.42476215958595276\n",
      "Absolute Difference/Train: 0.508585512638092\n",
      "Avg Loss/Train: 0.42476215958595276\n",
      "Avg Absolute Difference/Train: 0.508585512638092\n",
      "Lr/Train: 0.0009972609476841367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss/Val: 0.3222880959510803\n",
      "Avg Absolute Difference/Val: 0.45400914549827576\n",
      "Epoch [10/12] Val Loss: 0.3223, Val Diff: 0.454009145498276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:36<00:00, 36.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00011: adjusting learning rate of group 0 to 9.9669e-04.\n",
      "Loss/Train: 0.3666348457336426\n",
      "Absolute Difference/Train: 0.472565233707428\n",
      "Avg Loss/Train: 0.3666348457336426\n",
      "Avg Absolute Difference/Train: 0.472565233707428\n",
      "Lr/Train: 0.000996686382800198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:35<00:00, 35.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00012: adjusting learning rate of group 0 to 9.9606e-04.\n",
      "Loss/Train: 0.31753915548324585\n",
      "Absolute Difference/Train: 0.43969660997390747\n",
      "Avg Loss/Train: 0.31753915548324585\n",
      "Avg Absolute Difference/Train: 0.43969660997390747\n",
      "Lr/Train: 0.000996057350657239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "writer = SummaryWriter(f\"tb_logs/{now}\")\n",
    "\n",
    "VALIDATION_STEPS = 3\n",
    "\n",
    "model = model.to(device)\n",
    "best_avg_diff = 1000\n",
    "\n",
    "for idx, epoch in enumerate(range(last_epoch, NUM_EPOCHS)):\n",
    "    train(model, train_dataloader, optimizer, scheduler, total_loss, epoch, writer, log_perc=0.2)\n",
    "\n",
    "    if idx % VALIDATION_STEPS == 0:\n",
    "        average_diff = validate(model, val_dataloader, total_loss, epoch, writer)\n",
    "        if average_diff < best_avg_diff:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'loss_mse': loss_mse,\n",
    "                }, \"checkpoint.pth\")\n",
    "\n",
    "# Launch TensorBoard: `tensorboard --logdir=tb_logs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(model, test_dataloader, total_loss, epoch, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4578667,
     "sourceId": 7960124,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 18456,
     "sourceId": 22283,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30665,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
